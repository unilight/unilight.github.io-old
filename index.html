<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Wen-Chin Huang</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Wen-Chin Huang</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/profile-2020.png" alt="">
        </span>
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#news">News</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#publications">Publications</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#projects">Projects</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#talks">Talks</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#honors">Honors, awards and scholarship</a>
          </li>
        </ul>
      </div>


    </nav>

    <div class="container-fluid p-0">

      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="about">
        <div class="my-auto">
          <h1 class="mb-0">
            <span class="text-primary">Wen-Chin</span>
            Huang
          </h1>
          <div class="subheading mb-5">
            黃文勁<br>
            Nagoya, Japan. <br>
            Originated from Taipei, Taiwan.<br>
            <a href="mailto:wen.chinhuang@g.sp.m.is.nagoya-u.ac.jp">wen.chinhuang@g.sp.m.is.nagoya-u.ac.jp</a>
          </div>
          <p class="mb-5">
              I am a master student supervised by Prof. Tomoki Toda in <a href="https://www.toda.is.i.nagoya-u.ac.jp/">Toda Laboratory</a> at the <a href="https://www.i.nagoya-u.ac.jp/graduate-school-of-informatics/">Graduate School of Informatics</a> , Nagoya University starting from April 2019.<br>
              Prior to that, I was a research student in Toda Laboratory in Nagoya University with advisor Prof. Tomoki Toda. I was also a research assistant at the Institute of Information Science (IIS) in Academia Sinica, Taipei, Taiwan with advisor Prof. Hsin-Min Wang.<br>
              I received my Bachelor's degree from National Taiwan University in June 2018, majoring in Computer Science and Information Engineering (CSIE).<br>
              My research interests include speech processing, natural language processing and machine learning.<br>
              In particular, I am currently working on speech synthesis and voice conversion using deep generative models.<br>
              <br>
              I also love street dancing (locking). My team participated in a national dance contest. Check out the <a href="https://www.youtube.com/watch?v=7kfGe7zuQ5g">video</a>.<br>
              <br>
              Last update: 2020.07.29
          </p>

          <ul class="list-inline list-social-icons mb-0">
              <li class="list-inline-item">
                <a href="https://scholar.google.com/citations?user=g71mJO4AAAAJ&hl=en">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-google fa-stack-1x fa-inverse"></i>
                  </span>
                  Google Scholar
                </a>
              </li>
            <li class="list-inline-item">
              <a href="https://www.linkedin.com/in/wen-chin-vincent-huang-0b39ab138">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                </span>
                Linkedin
              </a>
            </li>
            <li class="list-inline-item">
              <a href="https://github.com/unilight">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
                Github
              </a>
            </li>
            <li class="list-inline-item">
              <a href="./CV-wchuang.pdf">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-file fa-stack-1x fa-inverse"></i>
                </span>
                CV
              </a>
            </li>
          </ul>
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="news">
        <div class="my-auto">
          <h2>News</h2>
          <h3>2020</h3>
          <li> <b>2020.07</b> The implementation of <a href="https://github.com/espnet/espnet/tree/master/egs/arctic/vc1">Transformer-VC</a> is open-sourced on <b>ESPnet</b>.</li>
          <li> <b>2020.07</b> One paper [<a href="https://unilight.github.io/Publication-Demos/publications/transformer-vc/">Transformer-VC</a>] was accepted to <b>Interspeech 2020</b>.</li>
          <li> <b>2020.06</b> One journal paper [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0885230820300474">ASVspoof 2019 database</a>] was accepted to the <b>Computer Speech & Language</b>.</li>
          <li> <b>2020.03</b> I am co-organizing the <a href="http://www.vc-challenge.org/">Voice Conversion Challenge 2020</a>. I developed a <a href="https://github.com/espnet/espnet/tree/master/egs/vcc20">seq-to-seq baseline model based on Cascade ASR + TTS w/ ESPnet</a>. </li>
          <li> <b>2020.01</b> One journal paper [<a href="https://arxiv.org/pdf/2001.07849.pdf">CDVAE-CLS-GAN</a>] was accepted to the <b>IEEE Transactions on Emerging Topics in Computational Intelligence</b>.</li>
          <br>

          <h3>2019</h3>
          <li> <b>2019.12</b> One paper [<a href="https://arxiv.org/pdf/1912.06813.pdf">Transformer-VC</a>] was submited to <b>arXiv</b>.</li>
          <li> <b>2019.11</b> I helped the construction of the <a href="https://arxiv.org/abs/1911.01601">ASVspoof 2019 database</a> and the paper was submited to <b>arXiv</b> and <b>Computer Speech & Language</b>.</li>
          <li> <b>2019.08</b> I started my internship at <b>NTT Communication Science Laboratories, NTT Corporation</b>.</li>
          <li> <b>2019.07</b> I received the <b>JEES・Docomo Scholarship for International Students</b>, 2019. (Two years of funding)</li>
          <li> <b>2019.07</b> We released the <a href="https://github.com/unilight/cdvae-vc">official implementation</a> of various CDVAE-based VC models.</li>
          <li> <b>2019.07</b> One paper [<a href="https://unilight.github.io/Publication-Demos/publications/ssw10/index.html">DWM-RES</a>] was accepted to <b>SSW10</b>.</li>
          <li> <b>2019.06</b> I received the travel grant for ISCA and Interspeech 2019.</li>
          <li> <b>2019.06</b> Two papers [<a href="https://unilight.github.io/Publication-Demos/publications/f0-fcn-cdvae/index.html">F0-FCN-CDVAE</a>][<a href="https://arxiv.org/pdf/1904.08352">MOSNet</a>] were accepted to <b>Interspeech 2019</b>.</li>
          <li> <b>2019.06</b> One paper [<a href="https://unilight.github.io/VAE-WNV-VC-Demo/">VAE-WNV-VC</a>] was accepted to <b>EUSIPCO 2019</b>.</li>
          <li> <b>2019.04</b> Two papers [<a href="https://arxiv.org/pdf/1905.00615">F0-FCN-CDVAE</a>] [<a href="https://arxiv.org/pdf/1904.08352">MOSNet</a>] were submited to <b>arXiv</b>.</li>
          <li> <b>2019.04</b> I started MS studies in Toda Laboratory at Nagoya University with advisor Prof. Tomoki Toda.</li>
          <br>

          <h3>2018</h3>
          <li> <b>2018.11</b> I won the <b>Best Student Paper Award</b> in <b>ISCSLP 2018</b>!</li>
          <li> <b>2018.10</b> One paper [<a href="https://unilight.github.io/VAE-WNV-VC-Demo/">VAE-WNV-VC</a>] was submited to <b>arXiv</b>.</li>
          <li> <b>2018.09</b> I enrolled in a research student program in Toda Laboratory at Nagoya University with advisor Prof. Tomoki Toda.</li>
          <li> <b>2018.08</b> One paper [<a href="http://www.iis.sinica.edu.tw/papers/whm/21990-F.pdf">WN-VC</a>] was accepted to <b>ROCLING 2018</b>.</li>
          <li> <b>2018.08</b> My first paper [<a href="https://unilight.github.io/CDVAE-Demo/">CDVAE-VC</a>] was accepted to <b>ISCSLP 2018</b>!</li>
          <li> <b>2018.06</b> I received my B.S. in Computer Science and Information Engineering from <b>National Taiwan University</b>.</li>
          <li> <b>2018.05</b> I was invited by <a href="http://faculty.csie.ntust.edu.tw/~kychen/">Prof. Kuan-Yu (Menphis) Chen</a> to give a lecture about Machine Comprehension with Deep Learning in the course <i>CS3048701: Deep Learning for Natural Language Processing</i> in National Taiwan University of Science and Technology.</li>
        </div>

      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="publications">
        <div class="my-auto">
          <h2 class="mb-5">Selected Publications</h2>
          <!---------->
          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h4 class="mb-0">Unsupervised Representation Disentanglement using Cross Domain Features and Adversarial Learning in Variational Autoencoder based Voice Conversion</h4>
                <i><b>Wen-Chin Huang</b>, Hao Luo, Hsin-Te Hwang, Chen-Chou Lo, Yu-Huai Peng, Yu Tsao, Hsin-Min Wang</i><br>
                IEEE Transactions on Emerging Topics in Computational Intelligence<br>
                arXiv:2001.07849, Janurary 2020<br>
                [<a href="https://unilight.github.io/Publication-Demos/publications/TETCI/index.html">Demo</a>][<a href="https://arxiv.org/pdf/2001.07849.pdf">Paper</a>]<br>
              </p>
            </div>
            <div class="resume-date text-md-right">
              <img src="https://unilight.github.io/Publication-Demos/publications/TETCI/imgs/proposed.png" width=250 align="right"></a>
            </div>
          </div>
          <!---------->
          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h4 class="mb-0">Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using Transformer with Text-to-Speech Pretraining</h4>
                <i><b>Wen-Chin Huang</b>, Tomoki Hayashi, Yi-Chiao Wu, Hirokazu Kameoka, Tomoki Toda</i><br>
                Interspeech 2020<br>
                arXiv:1912.06813, December 2019<br>
                [<a href="https://github.com/espnet/espnet/tree/master/egs/arctic/vc1">Code</a>][<a href="https://unilight.github.io/Publication-Demos/publications/transformer-vc/">Demo</a>][<a href="https://arxiv.org/pdf/1912.06813.pdf">Paper</a>]<br>
              </p>
            </div>
            <div class="resume-date text-md-right">
              <img src="https://unilight.github.io/Publication-Demos/publications/transformer-vc/imgs/tts-pt-new.png" width=250 align="right"></a>
            </div>
          </div>
          <!---------->
          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h4 class="mb-0">Generalization of Spectrum Differential based Direct Waveform Modification for Voice Conversion</h4>
                <i><b>Wen-Chin Huang</b>, Yi-Chiao Wu, Kazuhiro Kobayashi, Yu-Huai Peng, Hsin-Te Hwang, Patrick Lumban Tobing, Tomoki Toda, Yu Tsao, Hsin-Min Wang</i><br>
                SSW10<br>
                arXiv:1907.11898, July 2019<br>
                [<a href="https://unilight.github.io/Publication-Demos/publications/ssw10/index.html">Demo</a>][<a href="https://www.isca-speech.org/archive/SSW_2019/pdfs/SSW10_P_1-4.pdf">Paper</a>]</a><br>
              </p>
            </div>
            <div class="resume-date text-md-right">
              <img src="https://unilight.github.io/Publication-Demos/publications/ssw10/imgs/proposed.png" width=250 align="right"></a>
            </div>
          </div>
          <!---------->
          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h4 class="mb-0">Investigation of F0 conditioning and Fully Convolutional Networks in Variational Autoencoder based Voice Conversion</h4>
                <i><b>Wen-Chin Huang</b>, Yi-Chiao Wu,  Chen-Chou Lo, Patrick Lumban Tobing, Tomoki Hayashi, Kazuhiro Kobayashi, Tomoki Toda, Yu Tsao, Hsin-Min Wang</i><br>
                Interspeech 2019<br>
                arXiv:1905.00615, May 2019<br>
                [<a href="https://github.com/unilight/cdvae-vc">Code</a>][<a href="https://unilight.github.io/Publication-Demos/publications/f0-fcn-cdvae/index.html">Demo</a>][<a href="https://arxiv.org/pdf/1905.00615">Paper</a>]<br>
              </p>
            </div>
            <div class="resume-date text-md-right">
              <img src="https://unilight.github.io/Publication-Demos/publications/f0-fcn-cdvae/imgs/f0vae.png" width=250 align="right"></a>
            </div>
          </div>
          <!---------->
          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h4 class="mb-0"> Refined WaveNet Vocoder for Variational Autoencoder Based Voice Conversion</h4>
              <i><b>Wen-Chin Huang</b>, Yi-Chiao Wu, Hsin-Te Hwang, Patrick Lumban Tobing, Tomoki Hayashi, Kazuhiro Kobayashi, Tomoki Toda, Yu Tsao, Hsin-Min Wang</i><br>
              EUSIPCO 2019<br>
              arXiv:1811.11078, November 2018<br>
              [<a href="https://unilight.github.io/VAE-WNV-VC-Demo/">Demo</a>][<a href="https://arxiv.org/pdf/1811.11078.pdf">Paper</a>]<br>
              <br>
            </div>
            <div class="resume-date text-md-right">
                  <img src="https://unilight.github.io/VAE-WNV-VC-Demo/imgs/proposed.png" width=250 align="right"></a>
            </div>
          </div>
          <!---------->
          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h4 class="mb-0">WaveNet Vocoder and its Applications in Voice Conversion</h4>
                <i><b>Wen-Chin Huang</b>, Chen-Chou Lo, Hsin-Te Hwang, Yu Tsao, Hsin-Min Wang</i><br>
                ROCLING 2018<br>
                [<a href="https://www.iis.sinica.edu.tw/papers/whm/21990-F.pdf">Paper</a>]<br>
              </div>
              </p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">
              </span>
            </div>
          <!---------->
          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h4 class="mb-0">Voice Conversion Based on Cross-Domain Features Using Variational Auto Encoders</h4>
              <i><b>Wen-Chin Huang</b>, Hsin-Te Hwang, Yu-Huai Peng, Yu Tsao, Hsin-Min Wang</i><br>
              ISCSLP 2018<br>
              arXiv:1808.09634, August 2018<br>
              [<a href="https://github.com/unilight/cdvae-vc">Code</a>][<a href="https://unilight.github.io/CDVAE-Demo/">Demo</a>][<a href="https://arxiv.org/pdf/1808.09634.pdf">Paper</a>]<br>
              <br>
            </div>
            <div class="resume-date text-md-right">
                  <img src="https://unilight.github.io/CDVAE-Demo/imgs/cdvae_model.png" width=250 align="right"></a>
            </div>
          <!---------->
        </div>

      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="projects">
        <div class="my-auto">
          <h2 class="mb-5">(Side?) Projects</h2>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Generative Compression (NCode) implementation in Tensorflow</h3>
              <div class="subheading mb-3">Video Commmunication term project</div>
              <p>This repository is a term project for the <i>Video Communication</i> course I took in my senior year in NTU. The model, called <i>NCode</i>, was designed for generative compression, i.e. to utilize deep generative models for (image) compression. It used the idea of GAN and transfer learning (VGGNet) to achieve graceful compression.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">First released in 2018.06<br>[<a href="https://github.com/unilight/Generative-Compression">Code</a>]</span>
            </div>
          </div>


        <div class="resume-item d-flex flex-column flex-md-row mb-5">
          <div class="resume-content mr-auto">
            <h3 class="mb-0">R-Net Implementation in Tensorflow</h3>
            <div class="subheading mb-3">Intern project in IIS, Academia Sinica.</div>
            <div>This repository is a Tensorflow implementation of R-NET, a neural network designed to solve the Question Answering (QA) task. This implementation is specifically designed for SQuAD, a large-scale dataset drawing attention in the field of QA recently.</div>
          </div>
          <div class="resume-date text-md-right">
            <span class="text-primary">First released in 2017.08<br>[<a href="https://github.com/unilight/R-NET-in-Tensorflow">Code</a>]</span>
          </div>
        </div>
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="talks">
        <div class="my-auto">
          <h2 class="mb-5">Talks</h2>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Machine Comprehension with Deep Learning</h3>
              <div class="subheading mb-3">National Taiwan University of Science and Technology.</div>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">2018.05</span>
            </div>
          </div>

        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="honors">
        <div class="my-auto">
          <h2>Honors, awards and scholarship</h2>
          <li> JEES・Docomo Scholarship for International Students, 2019.</li>
          <li> Travel Grant, ISCA and Interspeech 2019</li>
          <li> Best Student Paper Award, 2018 11th International Symposium on Chinese Spoken Language Processing (ISCSLP)</li>
        </div>

      </section>


    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.min.js"></script>

  </body>

</html>
